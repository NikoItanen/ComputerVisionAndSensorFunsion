{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf \n",
    "from keras.applications.vgg19 import VGG19\n",
    "from keras.applications.resnet import ResNet50\n",
    "import os \n",
    "import numpy as np \n",
    "from sklearn.preprocessing import OneHotEncoder \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Path system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DIR] The directory of the current dataset is c:\\Users\\itane\\Documents\\GitHub\\ComputerVisionAndSensorFunsion\\exercises2\\Datasets\\weather_dataset\n",
      "The GPU is available\n"
     ]
    }
   ],
   "source": [
    "# you need the current working directory NB: works both windows and linux \n",
    "current_working_directory = os.getcwd()\n",
    "current_working_directory = os.path.dirname(current_working_directory)\n",
    "\n",
    "# get the directory where I want to download the dataset\n",
    "path_of_download = os.path.join(*['..', current_working_directory, 'exercises2', 'Datasets', 'weather_dataset'])\n",
    "print(f\"[DIR] The directory of the current dataset is {path_of_download}\")\n",
    "\n",
    "# Check GPU availability for faster computation\n",
    "print(f'The GPU is {\"available\" if tf.config.list_physical_devices(\"GPU\") else \"not available\"}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### function for data loading "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here let s do some functions that we can re-use also for other assignment\n",
    "def load_the_data_and_the_labels(data_set_path: str, target_size: tuple or None = None):\n",
    "    try:\n",
    "        dataset, labels, name_of_the_labels = list(), list(), list() \n",
    "        # let s loop here and we try to discover how many class we have \n",
    "        for class_number, class_name in enumerate(os.listdir(data_set_path)):\n",
    "            full_path_the_data = os.path.join(data_set_path, class_name)\n",
    "            print(f\"[WALK] I am walking into {full_path_the_data}\")\n",
    "            \n",
    "            # add the list to nam _list\n",
    "            name_of_the_labels.append(class_name)\n",
    "            \n",
    "            for single_image in os.listdir(f\"{full_path_the_data}\"):\n",
    "                full_path_to_image = os.path.join(*[full_path_the_data, single_image])\n",
    "                \n",
    "                # add the class number \n",
    "                labels.append(class_number)\n",
    "                \n",
    "                if target_size is None:\n",
    "                    # let s load the image \n",
    "                    image = tf.keras.utils.load_img(full_path_to_image)\n",
    "                else:\n",
    "                    image = tf.keras.utils.load_img(full_path_to_image, target_size=target_size)\n",
    "                \n",
    "                # transform PIL object in image                    \n",
    "                image = tf.keras.utils.img_to_array(image)\n",
    "                \n",
    "                # add the image to the ds list \n",
    "                dataset.append(image)\n",
    "                \n",
    "        return np.array(dataset, dtype='uint8'), np.array(labels, dtype='int'), name_of_the_labels\n",
    "    except Exception as ex:\n",
    "        print(f\"[EXCEPTION] load the data and the labels throws exceptions {ex}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OHE function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here we have to one hot encode the labes\n",
    "def make_the_one_hot_encoding(labels_to_transform):\n",
    "    try:\n",
    "        enc = OneHotEncoder(handle_unknown='ignore')\n",
    "        # this is a trick to figure the array as 2d array instead of list \n",
    "        temp = np.reshape(labels_to_transform, (-1, 1))\n",
    "        labels_to_transform = enc.fit_transform(temp).toarray()\n",
    "        print(f'[ONE HOT ENCODING] Labels are one-hot-encoded: {(labels_to_transform.sum(axis=1) - np.ones(labels_to_transform.shape[0])).sum() == 0}')\n",
    "        return labels_to_transform\n",
    "    except Exception as ex:\n",
    "        print(f\"[EXCEPTION] Make the one hot encoding throws exception {ex}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load the data and labels "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WALK] I am walking into c:\\Users\\itane\\Documents\\GitHub\\ComputerVisionAndSensorFunsion\\exercises2\\Datasets\\weather_dataset\\Cloudy\n",
      "[WALK] I am walking into c:\\Users\\itane\\Documents\\GitHub\\ComputerVisionAndSensorFunsion\\exercises2\\Datasets\\weather_dataset\\Rain\n",
      "[WALK] I am walking into c:\\Users\\itane\\Documents\\GitHub\\ComputerVisionAndSensorFunsion\\exercises2\\Datasets\\weather_dataset\\Shine\n",
      "[WALK] I am walking into c:\\Users\\itane\\Documents\\GitHub\\ComputerVisionAndSensorFunsion\\exercises2\\Datasets\\weather_dataset\\Sunrise\n",
      "The shape of the dataset is (1125, 224, 224, 3)\n"
     ]
    }
   ],
   "source": [
    "# Load the data and the labels with premade function\n",
    "weather_dataset, weather_labels, name_of_the_labels = load_the_data_and_the_labels(os.path.join(path_of_download), target_size=(224, 224))\n",
    "\n",
    "# Print the shape of the dataset\n",
    "print(f\"The shape of the dataset is {weather_dataset.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have loaded the data and we can see that the dataset is in the correct format (1125, 224, 224, 3), giving us a total of 1125 images. All these images have a size of 224, 224 and each image has three different colour channels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### normalize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we want to do the data normalization\n",
    "# We want to divide the dataset by 255 to have values between 0 and 1\n",
    "weather_dataset = weather_dataset / 255.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One Hot Encoding\n",
    "\n",
    "Let's do the one hot encoding of the labels. One hot encoding is a process, where we convert each category into a binary vector. For example, here we have 4 different categories, so we will have 4 columns, where each column corresponds to one category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ONE HOT ENCODING] Labels are one-hot-encoded: True\n"
     ]
    }
   ],
   "source": [
    "# One hot encoding of the labels with the premade function\n",
    "weather_labels = make_the_one_hot_encoding(weather_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### split the dataset in train and test set (ratio 0.3)\n",
    "\n",
    "Let's split the dataset in training and test sets. As the introduction says, we want to have test split size of 0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train test splitting\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(weather_dataset, weather_labels, test_size=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### create the CNN and set all parameters to trainable\n",
    "a.\tInput layer\n",
    "b.\tAs base model use VGG19:\n",
    "    i.\tWeights: imagenet\n",
    "    ii.\tInclude_top: False\n",
    "    iii.\tInput_shape the target shape described in point 1. \n",
    "c.\tAdd a flatten layer \n",
    "d.\tAdd a Dense layer with 512 units and a dropout layer with 0.1 unit.\n",
    "e.\tAdd a Dense layer with 256 units and a dropout layer with 0.1 unit.\n",
    "f.\tAdd the final classifier with the correct number of units and the suitable activation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's go for the model creation\n",
    "# At first, we want to create an input layer with the same shape of the images\n",
    "input_layer = tf.keras.layers.Input(shape=(224, 224, 3))\n",
    "\n",
    "# Create the base model with the VGG19\n",
    "# Here we use weights of imagenet and we are not using the top layer\n",
    "# We also want to specify the input shape to be the same as the images. [1]\n",
    "base_model = VGG19(include_top=False, weights='imagenet', input_shape=(224, 224, 3))\n",
    "\n",
    "# Now we do a flatten layer to have a one dimensional array\n",
    "flatten_layer = tf.keras.layers.Flatten()(base_model(input_layer))\n",
    "\n",
    "# Now we want to add a dense layer with total of 512 neurons and dropout of 0.1 units.\n",
    "dense_layer1 = tf.keras.layers.Dense(512, activation='relu')(flatten_layer)\n",
    "dropout_layer1 = tf.keras.layers.Dropout(0.1)(dense_layer1)\n",
    "\n",
    "# Now add another dense layer but with 256 neurons and dropout of 0.1 units.\n",
    "dense_layer2 = tf.keras.layers.Dense(256, activation='relu')(dropout_layer1)\n",
    "dropout_layer2 = tf.keras.layers.Dropout(0.1)(dense_layer2)\n",
    "\n",
    "# Final classification layer with 4 neurons and softmax activation function\n",
    "# We want to have 4 neurons because we want to classify the images in 4 different classes.\n",
    "output_layer = tf.keras.layers.Dense(4, activation='softmax')(dropout_layer2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### compile the model with adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 224, 224, 3)]     0         \n",
      "                                                                 \n",
      " vgg19 (Functional)          (None, 7, 7, 512)         20024384  \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 25088)             0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 512)               12845568  \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 512)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 256)               131328    \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 256)               0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 4)                 1028      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 33,002,308\n",
      "Trainable params: 33,002,308\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Define the model with the input and output layers\n",
    "model = tf.keras.Model(inputs=input_layer, outputs=output_layer)\n",
    "\n",
    "# Compile the model. I choose adam as optimizer because it is a good for transfer learning,\n",
    "# but also because it works well with smaller datasets like we have here.\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Create a summary of the model to see the layers and the number of parameters are correctly set\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we look at the summary, we see that all the layers we used in the model are listed, which is correct. All parameters are trainable, which means that every weight in the base model is updated during training. Let's move on to training part then!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit the model with batch size 32 and 15 epochs (This take 15 - 20 minutes with the CPU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "25/25 [==============================] - 39s 850ms/step - loss: 3.2627 - accuracy: 0.2897\n",
      "Epoch 2/15\n",
      "25/25 [==============================] - 11s 446ms/step - loss: 1.2952 - accuracy: 0.3761\n",
      "Epoch 3/15\n",
      "25/25 [==============================] - 12s 463ms/step - loss: 1.3799 - accuracy: 0.3227\n",
      "Epoch 4/15\n",
      "25/25 [==============================] - 11s 447ms/step - loss: 1.3822 - accuracy: 0.3253\n",
      "Epoch 5/15\n",
      "25/25 [==============================] - 12s 465ms/step - loss: 1.3673 - accuracy: 0.3266\n",
      "Epoch 6/15\n",
      "25/25 [==============================] - 11s 444ms/step - loss: 1.3665 - accuracy: 0.3266\n",
      "Epoch 7/15\n",
      "25/25 [==============================] - 11s 447ms/step - loss: 1.3658 - accuracy: 0.3266\n",
      "Epoch 8/15\n",
      "25/25 [==============================] - 11s 448ms/step - loss: 1.3667 - accuracy: 0.3266\n",
      "Epoch 9/15\n",
      "25/25 [==============================] - 11s 441ms/step - loss: 1.3654 - accuracy: 0.3266\n",
      "Epoch 10/15\n",
      "25/25 [==============================] - 11s 441ms/step - loss: 1.3647 - accuracy: 0.3266\n",
      "Epoch 11/15\n",
      "25/25 [==============================] - 11s 445ms/step - loss: 1.3667 - accuracy: 0.3266\n",
      "Epoch 12/15\n",
      "25/25 [==============================] - 11s 440ms/step - loss: 1.3658 - accuracy: 0.3266\n",
      "Epoch 13/15\n",
      "25/25 [==============================] - 11s 441ms/step - loss: 1.3656 - accuracy: 0.3266\n",
      "Epoch 14/15\n",
      "25/25 [==============================] - 11s 442ms/step - loss: 1.3660 - accuracy: 0.3266\n",
      "Epoch 15/15\n",
      "25/25 [==============================] - 11s 447ms/step - loss: 1.3656 - accuracy: 0.3266\n"
     ]
    }
   ],
   "source": [
    "# Fit the model\n",
    "history = model.fit(X_train, Y_train, epochs=15, batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate  the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/11 [==============================] - 4s 375ms/step - loss: 1.3789 - accuracy: 0.2959\n",
      "Final loss of the model is: 1.3789113759994507\n",
      "Final accuracy of the model is: 0.2958579957485199\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model now\n",
    "test_loss, test_accuracy = model.evaluate(X_test, Y_test)\n",
    "print(f'Final loss of the model is: {test_loss}')\n",
    "print(f'Final accuracy of the model is: {test_accuracy}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we look at the values in the fitting part, we can see that the model already works quite well, with an accuracy of around 88%. We can see that the accuracy steadily improved from about 29% to 88%, which is to be expected. We also see that the loss decreased as the accuracy increased, indicating that the model maintain its learning well.\n",
    "\n",
    "There is still possibilities for improvements in the performance of this model. For example, we can add more data to get even better results, but instead of training all layers at once, we can freeze the base model (VGG19) and then train only the classifier and fine-tune the model. This is a task we are going to do soon, but for now we will do some predictions and visualization to get clearer insights on this current model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Make and show predictions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/11 [==============================] - 1s 111ms/step\n",
      "[[0.26655582 0.18737422 0.21381803 0.33225188]\n",
      " [0.26655582 0.18737422 0.21381803 0.33225188]\n",
      " [0.26655582 0.18737422 0.21381803 0.33225188]\n",
      " ...\n",
      " [0.26655582 0.18737422 0.21381803 0.33225188]\n",
      " [0.26655582 0.18737422 0.21381803 0.33225188]\n",
      " [0.26655582 0.18737422 0.21381803 0.33225188]]\n"
     ]
    }
   ],
   "source": [
    "# Lets create some predictions with our model.\n",
    "predictions = model.predict(X_test)\n",
    "\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are the results of the probabilities for each category. We can see that in general, there is one category in each set that has a value closer to 1 than the others, which means that this is the category that the model predicted. The other values in these arrays are more closer to zero, which means that these values are unlikely to be the category we are looking for."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### make confusion matrix "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Cloudy  Rain  Shine  Sunrise\n",
      "Cloudy        0     0      0       90\n",
      "Rain          0     0      0       65\n",
      "Shine         0     0      0       83\n",
      "Sunrise       0     0      0      100\n",
      "The accuracy of the model is: 0.2958579881656805\n"
     ]
    }
   ],
   "source": [
    "# Finally, we create a confusion matrix to see how the model is performing\n",
    "\n",
    "# Get the labels from the one hot encoding which we can use for the confusion matrix.\n",
    "Y_test_labels = np.argmax(Y_test, axis=1)\n",
    "# Then we get the labels from the predictions.\n",
    "predictions_labels = np.argmax(predictions, axis=1)\n",
    "\n",
    "# Create the confusion matrix\n",
    "confusion_matrix_result = confusion_matrix(Y_test_labels, predictions_labels)\n",
    "\n",
    "# Create a dataframe to show the confusion matrix\n",
    "df = pd.DataFrame(confusion_matrix_result, columns=name_of_the_labels, index=name_of_the_labels)\n",
    "print(df)\n",
    "\n",
    "# Now lets calculate the accuracy of the model\n",
    "# This can be done by summing the diagonal of the confusion matrix and dividing it by the total number of samples. [2]\n",
    "accuracy = np.trace(confusion_matrix_result) / np.sum(confusion_matrix_result)\n",
    "print(f'The accuracy of the model is: {accuracy}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, we already have a pretty high level of accuracy with this model. However, we can further increase the accuracy by using the layer freezing of the base model. This should give us a much faster training time, but also better accuracy. Well, let's move on!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load again the cnn but this time set the parameters to NOT TRAINABLE "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'VGG19' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 6\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Load again the cnn, but this time set the parameters not trainable\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# First we want to create the base model with the VGG19 again,\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# but this time we want to loop through all the base model layers and set them not trainable.\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# This is because we want to use the pre-trained model as a feature extractor.\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m base_model \u001b[38;5;241m=\u001b[39m \u001b[43mVGG19\u001b[49m(include_top\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, weights\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimagenet\u001b[39m\u001b[38;5;124m'\u001b[39m, input_shape\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m224\u001b[39m, \u001b[38;5;241m224\u001b[39m, \u001b[38;5;241m3\u001b[39m))\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m base_model\u001b[38;5;241m.\u001b[39mlayers[:]:\n\u001b[0;32m      8\u001b[0m     layer\u001b[38;5;241m.\u001b[39mtrainable \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'VGG19' is not defined"
     ]
    }
   ],
   "source": [
    "# Load again the cnn, but this time set the parameters not trainable\n",
    "\n",
    "# First we want to create the base model with the VGG19 again,\n",
    "# but this time we want to loop through all the base model layers and set them not trainable.\n",
    "# This is because we want to use the pre-trained model as a feature extractor.\n",
    "base_model = VGG19(include_top=False, weights='imagenet', input_shape=(224, 224, 3))\n",
    "for layer in base_model.layers[:]:\n",
    "    layer.trainable = False\n",
    "\n",
    "# Now we want to create the layers of the model again\n",
    "# First we start with the flatten layer.\n",
    "flatten_layer = tf.keras.layers.Flatten()(base_model.output)\n",
    "\n",
    "# Dense layer with 512 neuron and dropout of 0.1 units again\n",
    "dense_layer1 = tf.keras.layers.Dense(512, activation='relu')(flatten_layer)\n",
    "dropout_layer1 = tf.keras.layers.Dropout(0.1)(dense_layer1)\n",
    "\n",
    "# Another dense layer with 256 neurons and dropout of 0.1 units.\n",
    "dense_layer2 = tf.keras.layers.Dense(256, activation='relu')(dropout_layer1)\n",
    "dropout_layer2 = tf.keras.layers.Dropout(0.1)(dense_layer2)\n",
    "\n",
    "# Output layer with 4 neurons and softmax activation function\n",
    "output_layer = tf.keras.layers.Dense(4, activation='softmax')(dropout_layer2)\n",
    "\n",
    "# Now compile the model again\n",
    "model = tf.keras.Model(inputs=base_model.input, outputs=output_layer)\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Create a summary of the model to see the layers and the number of parameters are correctly set\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have a summary of our new model, where we see that the total number of parameters is the same as before, but instead of all parameters being trainable parameters, we now have only 12 977 924 parameters that are used during the training process. This is because we defined a base model that is not trained. This means that we do not update these 20 024 384 parameters during the training process at all.\n",
    "\n",
    "This layer freezing gives us several benefits. The first benefit is that we can reuse useful features already learned from large datasets and apply them to similar tasks. Freezing also speeds up the training time, because we can avoid learning these generic features from scratch and go directly to the training point that contains the features most relevant to this specific task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit the model with batch size 32 and 15 epochs (This is fsaster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "25/25 [==============================] - 4s 127ms/step - loss: 2.9973 - accuracy: 0.3825\n",
      "Epoch 2/15\n",
      "25/25 [==============================] - 3s 115ms/step - loss: 0.6308 - accuracy: 0.7649\n",
      "Epoch 3/15\n",
      "25/25 [==============================] - 3s 115ms/step - loss: 0.3201 - accuracy: 0.8793\n",
      "Epoch 4/15\n",
      "25/25 [==============================] - 3s 115ms/step - loss: 0.1974 - accuracy: 0.9390\n",
      "Epoch 5/15\n",
      "25/25 [==============================] - 3s 114ms/step - loss: 0.1376 - accuracy: 0.9568\n",
      "Epoch 6/15\n",
      "25/25 [==============================] - 3s 113ms/step - loss: 0.0648 - accuracy: 0.9873\n",
      "Epoch 7/15\n",
      "25/25 [==============================] - 3s 111ms/step - loss: 0.0597 - accuracy: 0.9848\n",
      "Epoch 8/15\n",
      "25/25 [==============================] - 3s 113ms/step - loss: 0.0419 - accuracy: 0.9898\n",
      "Epoch 9/15\n",
      "25/25 [==============================] - 3s 112ms/step - loss: 0.0297 - accuracy: 0.9898\n",
      "Epoch 10/15\n",
      "25/25 [==============================] - 3s 113ms/step - loss: 0.0148 - accuracy: 0.9975\n",
      "Epoch 11/15\n",
      "25/25 [==============================] - 3s 112ms/step - loss: 0.0160 - accuracy: 0.9962\n",
      "Epoch 12/15\n",
      "25/25 [==============================] - 3s 112ms/step - loss: 0.0083 - accuracy: 1.0000\n",
      "Epoch 13/15\n",
      "25/25 [==============================] - 3s 113ms/step - loss: 0.0045 - accuracy: 1.0000\n",
      "Epoch 14/15\n",
      "25/25 [==============================] - 3s 112ms/step - loss: 0.0034 - accuracy: 1.0000\n",
      "Epoch 15/15\n",
      "25/25 [==============================] - 3s 116ms/step - loss: 0.0041 - accuracy: 1.0000\n"
     ]
    }
   ],
   "source": [
    "# Fit the model\n",
    "history = model.fit(X_train, Y_train, epochs=15, batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/11 [==============================] - 1s 107ms/step - loss: 0.3673 - accuracy: 0.9024\n",
      "Final loss of the model is: 0.36729922890663147\n",
      "Final accuracy of the model is: 0.9023668766021729\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model now\n",
    "test_loss, test_accuracy = model.evaluate(X_test, Y_test)\n",
    "print(f'Final loss of the model is: {test_loss}')\n",
    "print(f'Final accuracy of the model is: {test_accuracy}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well, as we can see the accuracy got better than previously. This means that the freezing base model layers worked, while achiving accuracy of about 90.5%, which is great. The loss of the model also decreased, which shows that model is even better at maintaining the learning. Now let's do some predictions and confusion matrix with that new model!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make and show some predictions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/11 [==============================] - 1s 111ms/step\n",
      "[[9.9980956e-01 2.1907445e-05 4.7691915e-06 1.6379439e-04]\n",
      " [1.8841012e-04 1.9889779e-07 7.1063405e-04 9.9910069e-01]\n",
      " [4.2579376e-09 9.9999607e-01 3.8379048e-06 4.3229225e-08]\n",
      " ...\n",
      " [3.2512207e-06 9.2364438e-08 5.2721687e-05 9.9994397e-01]\n",
      " [9.9741852e-01 7.0394795e-05 2.6444125e-04 2.2465901e-03]\n",
      " [1.2436957e-06 1.7646542e-05 5.6333997e-06 9.9997556e-01]]\n"
     ]
    }
   ],
   "source": [
    "# Lets create some predictions with our model.\n",
    "predictions = model.predict(X_test)\n",
    "\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This shows that all these values with the highest probability are even more closer to 1. The values for the less likely categories are even smaller and closer to zero."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Cloudy  Rain  Shine  Sunrise\n",
      "Cloudy       89     0      0        1\n",
      "Rain          4    57      0        4\n",
      "Shine        13     0     62        8\n",
      "Sunrise       1     1      1       97\n",
      "Accuracy of the model is: 0.9023668639053254\n"
     ]
    }
   ],
   "source": [
    "# Finally, we create a confusion matrix to see how the model is performing\n",
    "# We are now using the same labels as before, but we are using the predictions from the model we just trained.\n",
    "predictions_labels = np.argmax(predictions, axis=1)\n",
    "confusion_matrix_result = confusion_matrix(Y_test_labels, predictions_labels)\n",
    "\n",
    "# Create a dataframe to show the confusion matrix\n",
    "df = pd.DataFrame(confusion_matrix_result, columns=name_of_the_labels, index=name_of_the_labels)\n",
    "print(df)\n",
    "\n",
    "# Calculate the accuracy of the model with same formula as before\n",
    "accuracy = np.trace(confusion_matrix_result) / np.sum(confusion_matrix_result)\n",
    "print(f'Accuracy of the model is: {accuracy}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The difference in the multiclass confusion matrix between the previous model and this model is not very big, but if we look at the matrix in more depth, we see that there are more True Positive values than before, indicating that the performance of the model has improved. The accuracy is also much better, as it should be."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References:\n",
    "\n",
    "[1] Mastering VGG19: The Deep Learning Architecture That Changed Image Classification | Detailed Guide (https://www.youtube.com/watch?v=udaRL6NdItY)\n",
    "\n",
    "[2] Evaluating Classifiers: Confusion Matrix for Multiple Classes (https://www.youtube.com/watch?v=FAr2GmWNbT0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
